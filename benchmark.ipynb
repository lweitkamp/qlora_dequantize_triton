{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "\n",
    "from peft.utils.integrations import dequantize_module_weight\n",
    "from bitsandbytes.nn import Linear4bit\n",
    "\n",
    "from kernel_v0 import dequantize_triton_v0\n",
    "from kernel_v1 import dequantize_triton_v1\n",
    "from kernel_v2 import dequantize_triton_v2\n",
    "from kernel_v3 import dequantize_triton_v3\n",
    "from kernel_v4 import dequantize_triton_v4\n",
    "\n",
    "\n",
    "def bnb_Linear4bit(hd, m, dtype = torch.float16):\n",
    "    return Linear4bit(\n",
    "        hd, m, bias = None,\n",
    "        compute_dtype       = dtype,\n",
    "        compress_statistics = True,\n",
    "        quant_type          = \"nf4\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = bnb_Linear4bit(128 * 5, 512 * 5, dtype=torch.float16).to(\"cuda\")\n",
    "linear.quant_state.dtype = torch.float16\n",
    "\n",
    "out_reference = dequantize_module_weight(linear)\n",
    "out_triton_v0 = dequantize_triton_v0(linear.weight, linear.quant_state)\n",
    "out_triton_v1 = dequantize_triton_v1(linear.weight, linear.quant_state)\n",
    "out_triton_v2 = dequantize_triton_v2(linear.weight, linear.quant_state)\n",
    "out_triton_v3 = dequantize_triton_v3(linear.weight, linear.quant_state)\n",
    "out_triton_v4 = dequantize_triton_v4(linear.weight, linear.quant_state)\n",
    "\n",
    "torch.testing.assert_close(out_reference, out_triton_v0)\n",
    "torch.testing.assert_close(out_reference, out_triton_v1)\n",
    "torch.testing.assert_close(out_reference, out_triton_v2)\n",
    "torch.testing.assert_close(out_reference, out_triton_v3)\n",
    "torch.testing.assert_close(out_reference, out_triton_v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    triton.testing.Benchmark(\n",
    "            x_names=[\"M\", \"N\"],\n",
    "            x_vals=[(int(512 * i), int(512 * i)) for i in range(5, 40, 2)], \n",
    "            line_arg=\"provider\",\n",
    "            line_vals=[\"bitsandbytes\", \"triton_v0\", \"triton_v1\", \"triton_v2\", \"triton_v3\", \"triton_v4\"],\n",
    "            line_names=[\"bitsandbytes\", \"Triton V0\", \"Triton V1\", \"Triton V2\", \"Triton V3\", \"Triton V4\"],\n",
    "            styles=[(\"black\", \"--\"), (\"blue\", \"-\"), (\"red\", \"-\"), (\"orange\", \"-\"), (\"purple\", \"-\"), (\"yellow\", \"-\")],\n",
    "            ylabel=\"ms\",\n",
    "            plot_name=\"QLORA Dequantize Benchmark-bfloat16\",\n",
    "            args={\"bfloat16_instead_of_float16\": True},\n",
    "        ),\n",
    "    triton.testing.Benchmark(\n",
    "            x_names=[\"M\", \"N\"],\n",
    "            x_vals=[(int(512 * i), int(512 * i)) for i in range(5, 40, 2)], \n",
    "            line_arg=\"provider\",\n",
    "            line_vals=[\"bitsandbytes\", \"triton_v0\", \"triton_v1\", \"triton_v2\", \"triton_v3\", \"triton_v4\"],\n",
    "            line_names=[\"bitsandbytes\", \"Triton V0\", \"Triton V1\", \"Triton V2\", \"Triton V3\", \"Triton V4\"],\n",
    "            styles=[(\"black\", \"--\"), (\"blue\", \"-\"), (\"red\", \"-\"), (\"orange\", \"-\"), (\"purple\", \"-\"), (\"yellow\", \"-\")],\n",
    "            ylabel=\"ms\",\n",
    "            plot_name=\"QLORA Dequantize Benchmark-float16\",\n",
    "            args={\"bfloat16_instead_of_float16\": False},\n",
    "        ),\n",
    "]\n",
    "\n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(M, N, provider, bfloat16_instead_of_float16):\n",
    "    dtype = torch.bfloat16 if bfloat16_instead_of_float16 else torch.float16\n",
    "    linear = bnb_Linear4bit(M, N, dtype=dtype).to(\"cuda\")\n",
    "    linear.quant_state.dtype = dtype\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "\n",
    "    # Reference implementations \"cuda\" from bitsandbytes.\n",
    "    if provider == 'bitsandbytes':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: dequantize_module_weight(linear), quantiles=quantiles)\n",
    "\n",
    "    # Personal implementations (Triton).\n",
    "    if provider == 'triton_v0':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: dequantize_triton_v0(linear.weight, linear.quant_state), quantiles=quantiles)\n",
    "    if provider == 'triton_v1':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: dequantize_triton_v1(linear.weight, linear.quant_state), quantiles=quantiles)\n",
    "    if provider == 'triton_v2':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: dequantize_triton_v2(linear.weight, linear.quant_state), quantiles=quantiles)\n",
    "    if provider == 'triton_v3':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: dequantize_triton_v3(linear.weight, linear.quant_state), quantiles=quantiles)\n",
    "    if provider == 'triton_v4':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: dequantize_triton_v4(linear.weight, linear.quant_state), quantiles=quantiles)\n",
    "    return ms, max_ms, min_ms\n",
    "\n",
    "df = benchmark.run(show_plots=True, print_data=True, return_df=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
